{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn.metrics as skm\n",
    "import plotly.express as px\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data + set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import in data\n",
    "test = pd.read_csv(\"test-1.csv\")\n",
    "train = pd.read_csv(\"train-1.csv\")\n",
    "\n",
    "# split into output and input\n",
    "y = train['label']                  # y_train\n",
    "X = train.drop(['label'], axis = 1) # X_train\n",
    "\n",
    "y_test = test['label']\n",
    "X_test = test.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_logistic_regression: \n",
    "fits the model w/ func sigmoid and by updating weights w/ stochastic gradient descent <br>\n",
    "&emsp;<b> inputs </b>: epochs (# of epochs), learning_rate (learning rate/eta), X (input features), y (target feature)<br>\n",
    "&emsp;<b> output/return </b>: weights of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon for calculating cross-entropy loss\n",
    "EPSILON = 0.00000000000000000000000000001\n",
    "\n",
    "def fit_logistic_regression (epochs, learning_rate, X, y):\n",
    "    # initialize values\n",
    "    weights = [0] * (len(X.columns)) # vector of 0s\n",
    "    num_samples = X.shape[0]\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(0, int(epochs)): # run set number of epochs\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, num_samples):  # go through each sample in X\n",
    "            y_true = y[i]\n",
    "            x_values = X.iloc[i]\n",
    "            y_lin_pred = np.dot(weights, x_values)              # get linear portion of logistic regression\n",
    "            y_pred = 1/(1 + pow(math.e, -np.sum(y_lin_pred)))   # calculate final prediction\n",
    "            deriv = x_values * (y_pred - y_true)                # deriv of loss: formula calculated by hand! see md\n",
    "            weights = weights - learning_rate * deriv\n",
    "            # calculate binary cross-entropy loss bc it is a classification problem w/ binary outputs\n",
    "            epoch_loss = epoch_loss + (-(y_true * math.log(y_pred + EPSILON, math.e) + (1-y_true) * math.log((1-y_pred) + EPSILON, math.e)))\n",
    "        # loss for the final plot\n",
    "        loss_point = epoch_loss/num_samples\n",
    "        losses.append(loss_point)\n",
    "        # print info about each epoch\n",
    "        print(\"Epoch \" + str(epoch) + \": cross-entroy loss = \" + str(loss_point))\n",
    "    \n",
    "    # loss plot\n",
    "    epoch_num = np.arange(1, len(losses) + 1)\n",
    "    df_plot = pd.DataFrame({'Epoch': epoch_num, 'Loss': losses})\n",
    "\n",
    "    loss_plot = px.line(df_plot, x = 'Epoch', y = 'Loss', title = 'Loss over Epochs')\n",
    "    loss_plot.show()\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to get weights!\n",
    "weights = fit_logistic_regression(200, 0.1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_logistic_regression:\n",
    "<b> print_stats </b>: prints accuracy, precision, recall, f1, and confusion mtx based on given y_test + predictions <br>\n",
    "<b>predict_logistic_regression</b>: predicts X test classification based on given weights <br>\n",
    "&emsp;<b>inputs</b>: weights (from fit function), X_test, y_test <br>\n",
    "&emsp;<b>outputs</b>: nothing (but prints stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(y_test, y_preds):\n",
    "    print(\"Accuracy: \" + str(skm.accuracy_score(y_test, y_preds)))\n",
    "    print(\"Precision: \" + str(skm.precision_score(y_test, y_preds)))\n",
    "    print(\"Recall: \"+ str(skm.recall_score(y_test, y_preds)))\n",
    "    print(\"F1: \" + str(skm.f1_score(y_test, y_preds)))\n",
    "    print(\"Confusion Matrix: \" + str(skm.confusion_matrix(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression (weights, X_test):\n",
    "    num_samples = X_test.shape[0]\n",
    "    y_preds = []\n",
    "    for i in range(0, num_samples):                 # loop through all samples\n",
    "        lin_total = X_test.iloc[i].mul(weights).sum()       # get sample's values\n",
    "        pred = 1/(1 + pow(math.e, -lin_total))              # get prediction from sigmoid\n",
    "        y_preds.append(round(pred + EPSILON))               # round prediction and add to list of predictions\n",
    "        ## ^ note: add EPSILON bc round function rounds 0.5 down to 0, but it's supposed to round up\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets predictions + prints out stats (accuracy)\n",
    "predictions = predict_logistic_regression(weights, X_test)\n",
    "# print all stats\n",
    "print_stats(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare w/ sklearn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk_model = LogisticRegression()\n",
    "sk_model.fit(X, y)\n",
    "y_pred_sk = sk_model.predict(X_test)\n",
    "print_stats(y_test, y_pred_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observations + analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be significantly more ham samples than spam samples, likely biasing the model. The model would likely be better at discerning ham and spam if it was fed a more balanced data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percent spam in training set: \" + str(y.mean()))\n",
    "print(\"percent spam in test set: \" + str(y_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, there are many features in the data set (1364): likely, not all of them were important in the classification of ham/spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. model + results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is likely overfit, since the accuracy decreases as epochs increase too much: <br>\n",
    "200 epochs: Accuracy: 0.9641255605381166 <br>\n",
    "100 epochs: Accuracy: 0.9650224215246637<br>\n",
    "50 epochs: Accuracy: 0.9713004484304932 <br>\n",
    "\n",
    "The loss also barely decreases towards the end, as seen in the loss plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it seems like the model is much more likely to mistakenly classify a spam message as ham than mistakenly classify a message that's ham as spam. We see this from the confusion matrix, as there are significantly more false negatives than false positives, even though there were less spam datapoints. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix: \" + str(skm.confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This disparity likely stems from the inbalanced data set (mentioned above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an early stop mechanism could improve the results by preventing extreme overfitting. Additionally, eliminating noise features would increase model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
